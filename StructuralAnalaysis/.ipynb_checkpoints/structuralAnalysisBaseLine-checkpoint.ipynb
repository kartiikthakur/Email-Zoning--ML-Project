{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.jpg\" width=\"700\" />\n",
    "\n",
    "# Email Structural analysis using a Neural Network (NN) approach for TA2 layer of PANACEA project\n",
    "\n",
    "The purpose of this code is to present an approach for __detecting the central parts of an email__. The main idea, is to create a NN that can be used for detecting __greetings, content and signature elements__ associated to the textual content of a message.\n",
    "\n",
    "__Code main elements:__\n",
    "\n",
    "1. All the code was implemented in Python 3.7.1 https://www.python.org/\n",
    "2. The dataset used is a labeled subset of the well known Enron dataset and Apache mailing list. More information can be found here: https://github.com/HPI-Information-Systems/Quagga/blob/master/Datasets/DATA.md \n",
    "3. The  Python packages required to run the programs are the following:\n",
    "    * Jupyter notebook (Python interactive prompt) http://jupyter.org/index.html\n",
    "    * Matplotlib (visualization) https://matplotlib.org/\n",
    "    * Numpy (mathematical functions) http://www.numpy.org/\n",
    "    * Scipy (mathematical functions) https://www.scipy.org/\n",
    "    * NLTK (Natural Language Processing) https://www.nltk.org/\n",
    "    * sklearn (Machine Learning) http://scikit-learn.org/stable/\n",
    "    * Gensim https://radimrehurek.com/gensim/\n",
    "    \n",
    "__For more information see:__\n",
    "\n",
    "1. https://medium.com/datadriveninvestor/building-neural-network-using-keras-for-classification-3a3656c726c1\n",
    "2. https://github.com/HPI-Information-Systems/Quagga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predefined tool\n",
    "\n",
    "\n",
    "Check an existing package called \"Quagga\" for detecting the parts of an email: https://github.com/HPI-Information-Systems/QuaggaLib\n",
    "\n",
    "The package classify each text line into one of five types: __Header, Body, Body/Intro, Body/Outro, Body/Signature.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Quagga'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2b9d04022d08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m from Quagga import (Quagga, \n\u001b[0m\u001b[0;32m      2\u001b[0m             \u001b[0mEmailDirectoryReader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[0mListReaderRawEmailTexts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mListReaderExtractedBodies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mTempQuaggaReader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Quagga'"
     ]
    }
   ],
   "source": [
    "from Quagga import (Quagga, \n",
    "            EmailDirectoryReader, \n",
    "            ListReaderRawEmailTexts, \n",
    "            ListReaderExtractedBodies, \n",
    "            TempQuaggaReader, \n",
    "            ModelBuilder)\n",
    "from Quagga.Utils.BlockParser.BlockParser import BlockParser\n",
    "\n",
    "quagga = Quagga(ListReaderExtractedBodies(''), \n",
    "                output_dir='', # optional, saves to directory if set\n",
    "                model_builder=ModelBuilder(with_crf=True, zones=5, trainset='enron'), # optional\n",
    "                block_parser=BlockParser()) # optional\n",
    "email_body_text= (\"Hi,\\nI am trying to use the ConnectedComponent algorithm of GraphFrames but by default it\"+\n",
    "                 \"needs a checkpoint directory. As I am running my spark cluster with S3 as the DFS and do not\"+\n",
    "                 \" have access to HDFS file system I tried using a s3 directory as checkpoint directory but I run\"+\n",
    "                 \" into below exception:\\nRegards Sumit Chawla\\n Susan Priest\\nClasses of '77 and '81\")\n",
    "classified_lines = quagga.predict(email_body_text)\n",
    "print(classified_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our own classification model\n",
    "\n",
    "1\\. Obtain the current Jupyter notebook path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\School\\ILS\\StructuralAnalaysis\n"
     ]
    }
   ],
   "source": [
    "#Operating system functions for interacting with folders and files\n",
    "import os\n",
    "noteBookPath= os.getcwd()\n",
    "os.chdir(noteBookPath)\n",
    "print (noteBookPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Extract representative samples (all elements labeled as Body, Body/Intro, Body/Outro, Body/Signature) from Quagga training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: 'dataset' exists\n",
      "Directory: 'experiments' exists\n",
      "datasetPath variable dont exist, applying default one\n",
      "Training document already exist\n",
      "Test document already exist\n"
     ]
    }
   ],
   "source": [
    "#Encoding functions for opening files in the correct format\n",
    "import codecs\n",
    "import errno\n",
    "#Operating system functions for interacting with folders and files\n",
    "import os\n",
    "from os import listdir\n",
    "import sys\n",
    "import traceback\n",
    "#Mathematical functions to handle vectors\n",
    "import numpy as np\n",
    "#object-serializing functions\n",
    "import pickle\n",
    "#Calendar functions for timestamp\n",
    "import datetime\n",
    "import time\n",
    "#Encoding functions to handle Json files\n",
    "import json\n",
    "#Text maipulation functions (eliminate certain words for instance)\n",
    "import re\n",
    "\n",
    "#Check if the notebook path is already loaded\n",
    "try:  \n",
    "    noteBookPath\n",
    "except NameError:\n",
    "    print (\"NotebookPath variable dont exist, applying default one\")\n",
    "    noteBookPath= os.getcwd()\n",
    "    pass\n",
    "    \n",
    "#Check if dataset path exist\n",
    "datasetFolder=\"dataset\"\n",
    "try:\n",
    "    datasetsPath=os.path.join(noteBookPath,datasetFolder) \n",
    "    if not os.path.isdir(datasetsPath): raise Exception(\"'Dataset' folder dont exist\")\n",
    "    else: \n",
    "        print (\"Directory: '\"+ datasetFolder+\"' exists\")\n",
    "        pass\n",
    "except SystemExit as e:\n",
    "    print(e)\n",
    "\n",
    "#Create experiments folder if not exists\n",
    "dirName=\"experiments\"\n",
    "try:\n",
    "    os.mkdir(os.path.join(noteBookPath,dirName) )\n",
    "    print (\"Directory: '\"+dirName+\"' Created\") \n",
    "except OSError as e:\n",
    "    if e.errno == errno.EEXIST:\n",
    "        print (\"Directory: '\"+ dirName+\"' exists\")\n",
    "        pass\n",
    "    else:\n",
    "        raise \n",
    "except:\n",
    "    print (\"Unexpected error\")\n",
    "    pass  \n",
    "\n",
    "#Check if the experiments folder path exist\n",
    "try:  \n",
    "    experimentsPath\n",
    "except NameError:\n",
    "    print (\"datasetPath variable dont exist, applying default one\")\n",
    "    experimentsPath=os.path.join(noteBookPath,dirName)\n",
    "    pass\n",
    "\n",
    "#Python Script\n",
    "try:\n",
    "    \n",
    "    document = \"structuralAnaysisTraining.txt\"\n",
    "    filePath = os.path.join(experimentsPath,document)\n",
    "    \n",
    "    document2 = \"structuralAnaysisTest.txt\"\n",
    "    filePath2 = os.path.join(experimentsPath,document2)\n",
    "    \n",
    "    contentType=[\"Body\", \"Body/Intro\", \"Body/Outro\", \"Body/Signature\"]\n",
    "    \n",
    "    if not (os.path.isfile(filePath)):\n",
    "\n",
    "        print(\"Create trainig document\")\n",
    "        trainingSamples=[]\n",
    "        \n",
    "        textStatistics={\"Body\":[], \"Body/Intro\":[], \"Body/Outro\":[], \"Body/Signature\":[]}\n",
    "        \n",
    "        dataSetsTraining=[\"enronTrain\",\"apacheTrain\"]\n",
    "        for dataSetName in dataSetsTraining:\n",
    "            datasetPath=os.path.join(datasetsPath,dataSetName)\n",
    "            emailSamples = [f for f in listdir(datasetPath) if f.endswith(\".ann\")]\n",
    "            \n",
    "            for emailName in emailSamples:\n",
    "                emailPath=os.path.join(datasetPath,emailName)\n",
    "                with codecs.open(emailPath,\"r\",'utf8') as file:\n",
    "                    count=0\n",
    "                    data = json.load(file)\n",
    "                    for elements in data[\"denotations\"]:\n",
    "                        if elements[\"type\"] in contentType:\n",
    "                            text=elements[\"text\"].lower()\n",
    "                            text=re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''',\"\", text)\n",
    "                            text=re.sub(r\"\\S*@\\S*\\s?\",\"\", text)\n",
    "                            text=re.sub(r\"[^A-Za-z0-9\\n]+\",\" \", text)\n",
    "                            text=text.split(\"\\n\")\n",
    "                            text=[(i, x) for i, x in enumerate(text, 1)]\n",
    "                            temp=text[-1][0]\n",
    "                            text=[x for x in text if len(x[1])>1]\n",
    "                            \n",
    "                            #Statistics about the structure of the email \n",
    "                            for x in text:\n",
    "                                if elements[\"type\"]==\"Body\": \n",
    "                                    textStatistics[\"Body\"].append((count+x[0],len(x[1])))\n",
    "                                elif elements[\"type\"]==\"Body/Intro\": \n",
    "                                    textStatistics[\"Body/Intro\"].append((count+x[0],len(x[1])))\n",
    "                                elif elements[\"type\"]==\"Body/Outro\": \n",
    "                                    textStatistics[\"Body/Outro\"].append((count+x[0],len(x[1])))\n",
    "                                elif elements[\"type\"]==\"Body/Signature\": \n",
    "                                    textStatistics[\"Body/Signature\"].append((count+x[0],len(x[1])))    \n",
    "                            \n",
    "                            #Write training samples to a file\n",
    "                            text=[emailName+\" @@@ \"+elements[\"type\"]+\" @@@ \"+str(count+x[0]) +\" @@@ \"+str(len(x[1]))\n",
    "                                  +\" @@@ \"+x[1] for x in text]\n",
    "                            count=count+temp\n",
    "                            trainingSamples.extend(text)\n",
    "        \n",
    "        print(\"Average 'Body' length: \"+str(np.average([x[1] for x in textStatistics[\"Body\"]])))\n",
    "        print(\"Average 'Body' position: \"+str(np.average([x[0] for x in textStatistics[\"Body\"]])))\n",
    "        print(\"Average 'Body/Intro' length: \"+str(np.average([x[1] for x in textStatistics[\"Body/Intro\"]])))\n",
    "        print(\"Average 'Body/Intro' position: \"+str(np.average([x[0] for x in textStatistics[\"Body/Intro\"]])))\n",
    "        print(\"Average 'Body/Outro' length: \"+str(np.average([x[1] for x in textStatistics[\"Body/Outro\"]])))\n",
    "        print(\"Average 'Body/Outro' position: \"+str(np.average([x[0] for x in textStatistics[\"Body/Outro\"]])))\n",
    "        print(\"Average 'Body/Signature' length: \"+str(np.average([x[1] for x in textStatistics[\"Body/Signature\"]])))\n",
    "        print(\"Average 'Body/Signature' position: \"+str(np.average([x[0] for x in textStatistics[\"Body/Signature\"]])))\n",
    "        \n",
    "        with codecs.open(filePath, \"w\",'utf8') as file:    \n",
    "            [file.write(x+\"\\n\") for x in trainingSamples]\n",
    "    \n",
    "    else: print (\"Training document already exist\")\n",
    "        \n",
    "    if not (os.path.isfile(filePath2)): \n",
    "        \n",
    "        print(\"Create test document\")\n",
    "        testSamples=[]\n",
    "        \n",
    "        textStatistics={\"Body\":[], \"Body/Intro\":[], \"Body/Outro\":[], \"Body/Signature\":[]}\n",
    "        \n",
    "        dataSetsTest=[\"enronTest\",\"apacheTest\"]\n",
    "        for dataSetName in dataSetsTest:\n",
    "            datasetPath=os.path.join(datasetsPath,dataSetName)\n",
    "            emailSamples = [f for f in listdir(datasetPath) if f.endswith(\".ann\")]\n",
    "            for emailName in emailSamples:\n",
    "                emailPath=os.path.join(datasetPath,emailName)\n",
    "                with codecs.open(emailPath,\"r\",'utf8') as file:\n",
    "                    count=0\n",
    "                    data = json.load(file)\n",
    "                    for elements in data[\"denotations\"]:\n",
    "                        if elements[\"type\"] in contentType:\n",
    "                            text=elements[\"text\"].lower()\n",
    "                            text=re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''',\"\", text)\n",
    "                            text=re.sub(r\"\\S*@\\S*\\s?\",\"\", text)\n",
    "                            text=re.sub(r\"[^A-Za-z0-9\\n]+\",\" \", text)\n",
    "                            text=text.split(\"\\n\")\n",
    "                            text=[(i, x) for i, x in enumerate(text, 1)]\n",
    "                            temp=text[-1][0]\n",
    "                            text=[x for x in text if len(x[1])>1]\n",
    "                            text=[emailName+\" @@@ \"+elements[\"type\"]+\" @@@ \"+str(count+x[0]) +\" @@@ \"+str(len(x[1]))\n",
    "                                  +\" @@@ \"+x[1] for x in text]\n",
    "                            count=count+temp\n",
    "                            testSamples.extend(text)\n",
    "    \n",
    "        with codecs.open(filePath2, \"w\",'utf8') as file:    \n",
    "            [file.write(x+\"\\n\") for x in testSamples]    \n",
    "    \n",
    "    else: print (\"Test document already exist\")\n",
    "    \n",
    "except IOError as e:\n",
    "    print (\"Could not read file\")\n",
    "    print(traceback.format_exc())\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print (\"Unexpected error\")\n",
    "    print(traceback.format_exc())\n",
    "    print(e)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Extract character ngrams from project emails (JPL, Enron, APWG, etc.) in order to create distinct embedding models. These models will be used for generating training and test vectors associated to Quagga samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word embeddings functions to create word vectors\n",
    "import gensim \n",
    "#Encoding functions for opening files in the correct format\n",
    "import codecs\n",
    "import errno\n",
    "#Operating system functions for interacting with folders and files\n",
    "import os\n",
    "from os import listdir\n",
    "import sys\n",
    "import traceback\n",
    "#Mathematical functions to handle vectors\n",
    "import numpy as np\n",
    "#object-serializing functions\n",
    "import pickle\n",
    "#Calendar functions for timestamp\n",
    "import datetime\n",
    "import time\n",
    "#Text maipulation functions (eliminate certain words for instance)\n",
    "import re\n",
    "\n",
    "#Check if the notebook path is already loaded\n",
    "try:  \n",
    "    noteBookPath\n",
    "except NameError:\n",
    "    print (\"NotebookPath variable dont exist, applying default one\")\n",
    "    noteBookPath= os.getcwd()\n",
    "    pass\n",
    "\n",
    "\n",
    "#Check if dataset path exist\n",
    "datasetFolder=\"dataset\"\n",
    "try:\n",
    "    datasetsPath=os.path.join(noteBookPath,datasetFolder) \n",
    "    if not os.path.isdir(datasetsPath): raise Exception(\"'Dataset' folder dont exist\")\n",
    "    else: \n",
    "        print (\"Directory: '\"+ datasetFolder+\"' exists\")\n",
    "        pass\n",
    "except SystemExit as e:\n",
    "    print(e)\n",
    "\n",
    "#Create experiments folder if not exists\n",
    "dirName=\"experiments\"\n",
    "try:\n",
    "    os.mkdir(os.path.join(noteBookPath,dirName) )\n",
    "    print (\"Directory: '\"+dirName+\"' Created\") \n",
    "except OSError as e:\n",
    "    if e.errno == errno.EEXIST:\n",
    "        print (\"Directory: '\"+ dirName+\"' exists\")\n",
    "        pass\n",
    "    else:\n",
    "        raise \n",
    "except:\n",
    "    print (\"Unexpected error\")\n",
    "    pass  \n",
    "\n",
    "#Check if the experiments folder path exist\n",
    "try:  \n",
    "    experimentsPath\n",
    "except NameError:\n",
    "    print (\"datasetPath variable dont exist, applying default one\")\n",
    "    experimentsPath=os.path.join(noteBookPath,dirName)\n",
    "    pass\n",
    "\n",
    "#Python Script\n",
    "try:\n",
    "    \n",
    "    #Character ngram size\n",
    "    characterNgram=1\n",
    "    #Character embedding size vectors\n",
    "    featureNumbers=[100, 200, 300]\n",
    "    \n",
    "    document = \"friendFoeTraining.txt\"\n",
    "    filePath = os.path.join(experimentsPath,document)\n",
    "    \n",
    "    document2 = \"friendFoeTrainingCharacterNgramSize-\"+str(characterNgram)+\".txt\"\n",
    "    filePath2 = os.path.join(experimentsPath,document2)\n",
    "    \n",
    "    if (os.path.isfile(filePath)):\n",
    "        \n",
    "         if not (os.path.isfile(filePath2)): \n",
    "            \n",
    "            print(\"Extracting character ngrams of size: \"+str(characterNgram))\n",
    "            friendFoeTraining=[]\n",
    "            with codecs.open(filePath,\"r\",'ISO-8859-1') as file:\n",
    "                for line in file:\n",
    "                    elements=line.split(\" @@@ \")\n",
    "                    #Preprocess the text samples\n",
    "                    elements[1]=elements[1].replace(\"#\", \"\")\n",
    "                       #Eliminate URls\n",
    "                    elements[1]=re.sub(r\"http\\S+\",\"\", elements[1])\n",
    "                       #Eliminate usernames (which don't give any additional information for this analysis)\n",
    "                    elements[1]=re.sub(r\"@[^\\s]+\",\"\",elements[1])\n",
    "                    #text=text.replace(\" \", \"\")\n",
    "                    #Sseparate words in character ngrams for creating the embedding model\n",
    "                    #text=\" \".join([text[i:i+characterNgram] for i in range(len(text)-characterNgram+1)])\n",
    "                    friendFoeTraining.append(elements[0] + \" @@@ \" + elements[1])\n",
    "                    friendFoeTrainingTexts.append(elements[1])\n",
    "                    print(friendFoeTrainingTexts)\n",
    "                \n",
    "            with codecs.open(filePath2, \"w\",'ISO-8859-1') as file:    \n",
    "                #[file.write(x[0]+\" @@@ \"+x[1]+\"\\n\") for x in friendFoeTraining]\n",
    "                [file.write(text) for text in friendFoeTraining]\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            print(friendFoeTrainingTexts)\n",
    "            for number in featureNumbers:\n",
    "            \n",
    "                document3 = \"ngram-\"+str(characterNgram)+\"-EmbeddingModelSize-\"+str(number)\n",
    "                filePath3 = os.path.join(experimentsPath,document3)\n",
    "                \n",
    "                if not (os.path.isfile(filePath3)): \n",
    "                    \n",
    "                    print(\"Creating word embedding model of size: \"+str(number))\n",
    "                    #Traing wordEmbedding model\n",
    "                    model = gensim.models.Word2Vec(friendFoeTrainingTexts,size=number,window=10,min_count=2,workers=10)\n",
    "                    model.train(friendFoeTrainingTexts, total_examples=len(friendFoeTrainingTexts), epochs=10)\n",
    "                    #Serialize word-embedding object\n",
    "                    with open(filePath3, \"wb\") as file:\n",
    "                        pickle.dump(model, file)\n",
    "                else: print (\"Model: '\"+document3+\"' already exist\")        \n",
    "                        \n",
    "         else: \n",
    "            \n",
    "            print (\"Document: '\"+document2+\"' already exist\")\n",
    "            \n",
    "            with codecs.open(filePath2,\"r\",'ISO-8859-1') as file:\n",
    "                friendFoeTrainingTexts=[(line.split(\" @@@ \")[1]).split(\" \") for line in file]\n",
    "            \n",
    "            for number in featureNumbers:\n",
    "            \n",
    "                document3 = \"ngram-\"+str(characterNgram)+\"-EmbeddingModelSize-\"+str(number)\n",
    "                filePath3 = os.path.join(experimentsPath,document3)\n",
    "                \n",
    "                \n",
    "                if not (os.path.isfile(filePath3)): \n",
    "                    \n",
    "                    print(\"Creating word embedding model of size: \"+str(number))\n",
    "                    #Traing wordEmbedding model\n",
    "                    model = gensim.models.Word2Vec(friendFoeTrainingTexts,size=number,window=10,min_count=2,workers=10)\n",
    "                    model.train(friendFoeTrainingTexts, total_examples=len(friendFoeTrainingTexts), epochs=10)\n",
    "                    #Serialize word-embedding object\n",
    "                    with open(filePath3, \"wb\") as file:\n",
    "                        pickle.dump(model, file)\n",
    "                else: print (\"Model: '\"+document3+\"' already exist\") \n",
    "                 \n",
    "    else: print (\"Friend/foe training document dont exist\")\n",
    "         \n",
    "except IOError as e:\n",
    "    print (\"Could not read file\")\n",
    "    print(traceback.format_exc())\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print (\"Unexpected error\")\n",
    "    print(traceback.format_exc())\n",
    "    print(e)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Create training and test vectors for structural dataset considering the previously created embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: 'dataset' exists\n",
      "Directory: 'experiments' exists\n",
      "Create training vectors of size: 400\n",
      "Character embedding model dont exist or training vectors already created\n",
      "Create training vectors of size: 500\n",
      "Character embedding model dont exist or training vectors already created\n",
      "Create training vectors of size: 600\n",
      "Character embedding model dont exist or training vectors already created\n",
      "Create test vectors of size: 400\n",
      "Character embedding model dont exist or test vectors already created\n",
      "Create test vectors of size: 500\n",
      "Character embedding model dont exist or test vectors already created\n",
      "Create test vectors of size: 600\n",
      "Character embedding model dont exist or test vectors already created\n"
     ]
    }
   ],
   "source": [
    "#Word embeddings functions to create word vectors\n",
    "import gensim \n",
    "#Encoding functions for opening files in the correct format\n",
    "import codecs\n",
    "import errno\n",
    "#Operating system functions for interacting with folders and files\n",
    "import os\n",
    "from os import listdir\n",
    "import sys\n",
    "import traceback\n",
    "#Mathematical functions to handle vectors\n",
    "import numpy as np\n",
    "#object-serializing functions\n",
    "import pickle\n",
    "#Calendar functions for timestamp\n",
    "import datetime\n",
    "import time\n",
    "#Text maipulation functions (eliminate certain words for instance)\n",
    "import re\n",
    "\n",
    "#Check if the notebook path is already loaded\n",
    "try:  \n",
    "    noteBookPath\n",
    "except NameError:\n",
    "    print (\"NotebookPath variable dont exist, applying default one\")\n",
    "    noteBookPath= os.getcwd()\n",
    "    pass\n",
    "\n",
    "\n",
    "#Check if dataset path exist\n",
    "datasetFolder=\"dataset\"\n",
    "try:\n",
    "    datasetsPath=os.path.join(noteBookPath,datasetFolder) \n",
    "    if not os.path.isdir(datasetsPath): raise Exception(\"'Dataset' folder dont exist\")\n",
    "    else: \n",
    "        print (\"Directory: '\"+ datasetFolder+\"' exists\")\n",
    "        pass\n",
    "except SystemExit as e:\n",
    "    print(e)\n",
    "\n",
    "#Create experiments folder if not exists\n",
    "dirName=\"experiments\"\n",
    "try:\n",
    "    os.mkdir(os.path.join(noteBookPath,dirName) )\n",
    "    print (\"Directory: '\"+dirName+\"' Created\") \n",
    "except OSError as e:\n",
    "    if e.errno == errno.EEXIST:\n",
    "        print (\"Directory: '\"+ dirName+\"' exists\")\n",
    "        pass\n",
    "    else:\n",
    "        raise \n",
    "except:\n",
    "    print (\"Unexpected error\")\n",
    "    pass  \n",
    "\n",
    "#Check if the experiments folder path exist\n",
    "try:  \n",
    "    experimentsPath\n",
    "except NameError:\n",
    "    print (\"datasetPath variable dont exist, applying default one\")\n",
    "    experimentsPath=os.path.join(noteBookPath,dirName)\n",
    "    pass\n",
    "\n",
    "#Python Script\n",
    "try:\n",
    "    \n",
    "    document = \"structuralAnaysisTraining.txt\"\n",
    "    filePath = os.path.join(experimentsPath,document)\n",
    "\n",
    "    document2 = \"structuralAnaysisTest.txt\"\n",
    "    filePath2 = os.path.join(experimentsPath,document2)\n",
    "    \n",
    "    #Character ngram size\n",
    "    characterNgram=1\n",
    "    #Character embedding size vectors\n",
    "    featureNumbers=[100, 200, 300]\n",
    "    \n",
    "    #Training vectors creation (if needed)\n",
    "    if os.path.isfile(filePath):\n",
    "        \n",
    "        trainingTexts=[]\n",
    "        labels=[]\n",
    "        trainingLabels=[]\n",
    "        with codecs.open(filePath, \"r\", \"utf8\") as file: \n",
    "            for line in file:\n",
    "                try:\n",
    "                    elements=line.split(\" @@@ \")\n",
    "                    labels.append(elements[1])\n",
    "                    #text=elements[4].rstrip()\n",
    "                    trainingTexts.append((elements[4].rstrip()).split(\" \"))\n",
    "                except IndexError: pass\n",
    "                \n",
    "                \n",
    "        for label in labels:\n",
    "            if label==\"Body\":         trainingLabels.append([\"1\", \"0\", \"0\", \"0\"])\n",
    "            elif label==\"Body/Intro\": trainingLabels.append([\"0\", \"1\", \"0\", \"0\"])\n",
    "            elif label==\"Body/Outro\": trainingLabels.append([\"0\", \"0\", \"1\", \"0\"]) \n",
    "            else:                     trainingLabels.append([\"0\", \"0\", \"0\", \"1\"])\n",
    "        \n",
    "        \n",
    "        #Create character embeddings for training emails\n",
    "        for number in featureNumbers:\n",
    "            \n",
    "            print(\"Create training vectors of size: \"+str(number))\n",
    "            \n",
    "            document3 = \"Ngram-\"+str(characterNgram)+\"-EmbeddingModelSize-\"+str(number)\n",
    "            filePath3 = os.path.join(experimentsPath,document3)\n",
    "            \n",
    "            document4 = \"ngram-\"+str(characterNgram)+\"TrainingVectorsSize-\"+str(number)+\".txt\"\n",
    "            filePath4 = os.path.join(experimentsPath,document4)\n",
    "        \n",
    "            if (os.path.isfile(filePath3)) and not os.path.isfile(filePath4):\n",
    "                \n",
    "                with open(filePath3, \"rb\") as file: \n",
    "                    model= pickle.load(file,encoding=\"bytes\")\n",
    "                \n",
    "                #Create training vectors   \n",
    "                trainingVectors=[]\n",
    "                for text in trainingTexts:\n",
    "                    wordVectors=[]\n",
    "                    for word in text:\n",
    "                        try: ngramVectors.append(model[word])\n",
    "                        except: pass\n",
    "                    if len(ngramVectors)>=1:    \n",
    "                          wordVectors=np.array(wordVectors)    \n",
    "                          trainingVectors.append((wordVectors.mean(axis=0)).tolist())\n",
    "                    else: trainingVectors.append([0 for x in range(number)])\n",
    "                        \n",
    "                #Save train vectors in a text file\n",
    "                with codecs.open(filePath4, \"w\", 'ISO-8859-1') as file:   \n",
    "                    for vector,label in zip(trainingVectors,trainingLabels):\n",
    "                        file.write((\",\".join(label))+\",\"+(\",\".join([str(x) for x in vector]))+\"\\n\")\n",
    "                            \n",
    "            else: print (\"Character embedding model dont exist or training vectors already created\")\n",
    "                    \n",
    "    else: print (\"Training document: \"+document+\" dont exist\")\n",
    "        \n",
    "        \n",
    "    #Test vectors creation (if needed)\n",
    "    if os.path.isfile(filePath2):\n",
    "                       \n",
    "        testTexts=[]\n",
    "        labels=[]\n",
    "        testLabels=[]\n",
    "        with codecs.open(filePath2, \"r\", \"utf8\") as file: \n",
    "            for line in file:\n",
    "                try:\n",
    "                    elements=line.split(\" @@@ \")\n",
    "                    labels.append(elements[0])\n",
    "                    text=elements[1].rstrip()\n",
    "                    testTexts.append((elements[4].rstrip()).split(\" \"))\n",
    "                except IndexError: pass\n",
    "                 \n",
    "        for label in labels:\n",
    "            if label==\"Body\":         testLabels.append([\"1\", \"0\", \"0\", \"0\"])\n",
    "            elif label==\"Body/Intro\": testLabels.append([\"0\", \"1\", \"0\", \"0\"])\n",
    "            elif label==\"Body/Outro\": testLabels.append([\"0\", \"0\", \"1\", \"0\"]) \n",
    "            else:                     testLabels.append([\"0\", \"0\", \"0\", \"1\"]) \n",
    "                             \n",
    "        #Create character embeddings for test emails\n",
    "        for number in featureNumbers:\n",
    "            \n",
    "            print(\"Create test vectors of size: \"+str(number))\n",
    "            \n",
    "            document3 = \"Ngram-\"+str(characterNgram)+\"-EmbeddingModelSize-\"+str(number)\n",
    "            filePath3 = os.path.join(experimentsPath,document3)\n",
    "\n",
    "            document5 = \"ngram-\"+str(characterNgram)+\"TestVectorsSize-\"+str(number)+\".txt\"\n",
    "            filePath5 = os.path.join(experimentsPath,document5)\n",
    "        \n",
    "            if (os.path.isfile(filePath3)) and not os.path.isfile(filePath5):\n",
    "                \n",
    "                with open(filePath3, \"rb\") as file: \n",
    "                    model= pickle.load(file,encoding=\"bytes\")\n",
    "                    \n",
    "                #Create test vectors   \n",
    "                testVectors=[]\n",
    "                for text in testTexts:\n",
    "                    testVector=[]\n",
    "                    for word in text:\n",
    "                        try: testVector.append(model[word])\n",
    "                        except: pass\n",
    "                    if len(testVector)>=1:    \n",
    "                          testVector=np.array(testVector)    \n",
    "                          testVectors.append((testVector.mean(axis=0)).tolist())\n",
    "                    else: testVectors.append([0 for x in range(number)])\n",
    "                        \n",
    "                #Save test vectors in a text file\n",
    "                with codecs.open(filePath5, \"w\", 'ISO-8859-1') as file:   \n",
    "                    for vector,label in zip(testVectors,testLabels):\n",
    "                        file.write((\",\".join(label))+\",\"+(\",\".join([str(x) for x in vector]))+\"\\n\")               \n",
    "            \n",
    "            else: print (\"Character embedding model dont exist or test vectors already created\")\n",
    "                 \n",
    "    else: print (\"Test document: \"+document2+\" dont exist\")      \n",
    "\n",
    "except IOError as e:\n",
    "    print (\"Could not read file\")\n",
    "    print(traceback.format_exc())\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print (\"Unexpected error\")\n",
    "    print(traceback.format_exc())\n",
    "    print(e)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Create and test a Neural Network (Back propagation approach) using the training and test vectors previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: 'dataset' exists\n",
      "Directory: 'experiments' exists\n",
      "Character embeddings experiments using vectors of size: 400\n",
      "Unexpected error\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-9604847a0453>\", line 110, in <module>\n",
      "    trainingVectors=np.array(trainingVectors)\n",
      "MemoryError\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337) # for reproducibility\n",
    "#Neural network functions for creating a prediction model\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "#Evaluation metrics for checking classification model performance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#Operating system functions for interacting with folders and files\n",
    "import codecs\n",
    "import errno\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "#Check if the notebook path is already loaded\n",
    "try:  \n",
    "    noteBookPath\n",
    "except NameError:\n",
    "    print (\"NotebookPath variable dont exist, applying default one\")\n",
    "    noteBookPath= os.getcwd()\n",
    "    pass\n",
    "\n",
    "\n",
    "#Check if dataset path exist\n",
    "datasetFolder=\"dataset\"\n",
    "try:\n",
    "    datasetsPath=os.path.join(noteBookPath,datasetFolder) \n",
    "    if not os.path.isdir(datasetsPath): raise Exception(\"'Dataset' folder dont exist\")\n",
    "    else: \n",
    "        print (\"Directory: '\"+ datasetFolder+\"' exists\")\n",
    "        pass\n",
    "except SystemExit as e:\n",
    "    print(e)\n",
    "\n",
    "#Create experiments folder if not exists\n",
    "dirName=\"experiments\"\n",
    "try:\n",
    "    os.mkdir(os.path.join(noteBookPath,dirName) )\n",
    "    print (\"Directory: '\"+dirName+\"' Created\") \n",
    "except OSError as e:\n",
    "    if e.errno == errno.EEXIST:\n",
    "        print (\"Directory: '\"+ dirName+\"' exists\")\n",
    "        pass\n",
    "    else:\n",
    "        raise \n",
    "except:\n",
    "    print (\"Unexpected error\")\n",
    "    pass  \n",
    "\n",
    "#Check if the experiments folder path exist\n",
    "try:  \n",
    "    experimentsPath\n",
    "except NameError:\n",
    "    print (\"datasetPath variable dont exist, applying default one\")\n",
    "    experimentsPath=os.path.join(noteBookPath,dirName)\n",
    "    pass\n",
    "\n",
    "#Python Script\n",
    "try:\n",
    "    \n",
    "    #Parameters used for testing different Neural Network options\n",
    "    hiddenLayers=[[50,40,30,20,10],[50,25,10],[50,50],[16,8],[32,16,8],[8,8,8],[8,8],[4,4]]\n",
    "    batches=[x for x in range(20,110,5)]\n",
    "    epochs=[x for x in range(3,10,1)] \n",
    "    #Labels used to predict the structure of emails\n",
    "    structuralLabels=[\"Body\", \"Body/Intro\", \"Body/Outro\", \"Body/Signature\"]\n",
    "    #Mapping used for transforming values of the final NN layer to a specific structure category\n",
    "    structureMapping={0:\"Body\", 1:\"Body/Intro\", 2:\"Body/Outro\", 3:\"Body/Signature\"}\n",
    "    #Character ngram size\n",
    "    characterNgram=3\n",
    "    #Character embedding size vectors\n",
    "    featureNumbers=[400, 500, 600]\n",
    "    \n",
    "    #Iterate over each previously created Word-embedding file\n",
    "    for number in featureNumbers: \n",
    "        \n",
    "        print(\"Character embeddings experiments using vectors of size: \"+str(number))\n",
    "        \n",
    "        document = \"ResultsNgram-\"+str(characterNgram)+\"-EmbeddingSize-\"+str(number)+\".txt\"\n",
    "        filePath = os.path.join(experimentsPath,document)\n",
    "        \n",
    "        if not (os.path.isfile(filePath)):\n",
    "                \n",
    "            #List that have all the experimental results associate to a character embedding size\n",
    "            results=[]\n",
    "        \n",
    "            #Setting appropiate filepaths for experiments\n",
    "            document2 = \"ngram-\"+str(characterNgram)+\"TrainingVectorsSize-\"+str(number)+\".txt\"\n",
    "            filePath2 = os.path.join(experimentsPath,document2)\n",
    "        \n",
    "            document3 = \"ngram-\"+str(characterNgram)+\"TestVectorsSize-\"+str(number)+\".txt\"\n",
    "            filePath3 = os.path.join(experimentsPath,document3)\n",
    "            \n",
    "            if os.path.isfile(filePath2) and os.path.isfile(filePath3):\n",
    "                \n",
    "                #Test different variations of a Neural Network using training and test data\n",
    "                for layer in hiddenLayers:\n",
    "                    \n",
    "                    #Read training vectors (Enron, JPL, etc.)\n",
    "                    with codecs.open(filePath2,\"r\", \"ISO-8859-1\") as file:\n",
    "                        vectors=[line.replace('\\n','') for line in file]\n",
    "                        \n",
    "                    trainingVectors=[[float(feature) for feature in vector.split(\",\")[4:]] \n",
    "                                                for vector in vectors]\n",
    "                    trainingLabels=[[int(feature) for feature in vector.split(\",\")[:4]] \n",
    "                                                for vector in vectors]\n",
    "                    trainingVectors=np.array(trainingVectors) \n",
    "                    trainingLabels=np.array(trainingLabels)\n",
    "                    \n",
    "                    #Read test vectors (Enron, JPL, etc.)\n",
    "                    with codecs.open(filePath3,\"r\", \"ISO-8859-1\") as file:\n",
    "                        vectors=[line.replace('\\n','') for line in file]\n",
    "                    testVectors=[[float(feature) for feature in vector.split(\",\")[4:]] \n",
    "                                                for vector in vectors]\n",
    "                    testLabels=[[int(feature) for feature in vector.split(\",\")[:4]] \n",
    "                                                for vector in vectors]\n",
    "                    testVectors=np.array(testVectors) \n",
    "                    testLabels=np.array(testLabels)\n",
    "                    \n",
    "                    #Create the Neural Network architecture (Back-propagation approach)\n",
    "                    classifier = Sequential()\n",
    "                    #Hidden Layers\n",
    "                    classifier.add(Dense(layer[0], activation='relu', kernel_initializer='random_normal', input_dim=number))\n",
    "                    for element in layer[1:]:\n",
    "                        classifier.add(Dense(element, activation='relu', kernel_initializer='random_normal'))\n",
    "                        #Layer defined to avoid overfitting\n",
    "                        classifier.add(Dropout(0.2))\n",
    "                    #Output Layer\n",
    "                    classifier.add(Dense(4, activation='softmax', kernel_initializer='random_normal'))\n",
    "                    \n",
    "                    #Compiling the Neural Network\n",
    "                    classifier.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "                    \n",
    "                    #Choose the number of epochs\n",
    "                    for epoch in epochs:\n",
    "                        #Choose the number of batches to handle\n",
    "                        for batch in batches:\n",
    "\n",
    "                            #Create the prediction model\n",
    "                            classifier.fit(trainingVectors,trainingLabels, batch_size=batch, epochs=epoch,verbose=0)\n",
    "                            #Predict the age for each sample in the test dataset\n",
    "                            predictions = classifier.predict(testVectors)\n",
    "                            #Get the neuron id (0, 1 ,2 or 3) with the highest probability and then extract the age label\n",
    "                            predictedLabels=[structureMapping[np.argmax(x)] for x in predictions]\n",
    "                            expectedLabels=[structureMapping[np.argmax(x)] for x in testLabels]\n",
    "                            #Get model accuracy associated to each experiment\n",
    "                            accuracy=accuracy_score(expectedLabels, predictedLabels)\n",
    "                            \"\"\"\n",
    "                            Get F1-measure associated to each experiment\n",
    "                            \n",
    "                            F1 = 2 * (precision * recall) / (precision + recall)\n",
    "                            \n",
    "                            Calculate metrics for each label, and find their unweighted mean. This does not take \n",
    "                            label imbalance into account.\n",
    "                            \"\"\"\n",
    "                            f1=f1_score(expectedLabels, predictedLabels, average='macro')  \n",
    "                            #Get the confusion matrix associated to each experiment\n",
    "                            confusionMatrix=confusion_matrix(expectedLabels, predictedLabels, labels=structuralLabels)\n",
    "                            #Squish the matrix into a vector and then append all elements in a single string var\n",
    "                            confusionMatrix=\",\".join([str(x) for x in confusionMatrix.flatten()])\n",
    "                            #Print experimental results\n",
    "                            numberLayers=\"-\".join([str(x) for x in layer])\n",
    "                            res= (str(number)+\",\"+numberLayers+\",\"+str(epoch)+\",\"+str(batch)+\n",
    "                                 \",\"+str(accuracy)+\",\"+str(f1))\n",
    "                            print(res)      \n",
    "                            res=res+\",\"+confusionMatrix    \n",
    "                            results.append(res)\n",
    "\n",
    "                #Write experimental results to a file            \n",
    "                with codecs.open(filePath,\"w\", \"ISO-8859-1\") as file: \n",
    "                    [file.write(x+\"\\n\") for x in results]\n",
    "    \n",
    "            else: print (\"One of the following documents dont exist: \"+document2+\" or \"+document3)    \n",
    "            \n",
    "        else: print (\"The following document already exist: \"+document)\n",
    "\n",
    "except IOError as e:\n",
    "    print (\"Could not read file\")\n",
    "    print(traceback.format_exc())\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print (\"Unexpected error\")\n",
    "    print(traceback.format_exc())\n",
    "    print(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Obtain best experimental results according to F1, Accuracy and confusion matrix values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: 'dataset' exists\n",
      "Directory: 'experiments' exists\n",
      "Get results of document: ResultsNgram-3-EmbeddingSize-100.txt\n",
      "Get results of document: ResultsNgram-3-EmbeddingSize-200.txt\n",
      "Get results of document: ResultsNgram-3-EmbeddingSize-300.txt\n",
      "Get results of document: ResultsNgram-3-EmbeddingSize-400.txt\n",
      "Get results of document: ResultsNgram-3-EmbeddingSize-500.txt\n",
      "Get results of document: ResultsNgram-3-EmbeddingSize-600.txt\n",
      "\n",
      "Total number of experiments: 6048\n",
      "\n",
      "Character embedding best experiments according to F1 values\n",
      "400,50-50,4,35,0.897335701598579,0.4990003631011103,7313,49,155,95,122,35,1,1,188,5,156,1,249,1,0,74\n",
      "500,50-50,3,105,0.8911782119597395,0.487091395688954,7271,45,215,81,111,40,7,1,192,1,157,0,266,0,0,58\n",
      "500,50-50,4,25,0.8976909413854351,0.4839037656034657,7350,57,154,51,109,50,0,0,202,1,147,0,289,1,0,34\n",
      "500,50-25-10,4,60,0.8921255180580225,0.48292126357985354,7298,49,187,78,106,43,10,0,214,1,134,1,264,0,1,59\n",
      "100,50-50,4,20,0.8905861456483126,0.4825158497651963,7269,52,233,58,97,49,11,2,183,1,165,1,284,0,2,38\n",
      "600,50-25-10,3,40,0.8911782119597395,0.4784512036047211,7269,32,241,70,116,31,12,0,183,0,167,0,263,0,2,59\n",
      "600,50-50,3,85,0.888809946714032,0.47732426406026085,7249,39,227,97,108,32,18,1,191,1,157,1,253,2,1,68\n",
      "600,50-25-10,3,75,0.8924807578448787,0.4771740452870482,7299,51,217,45,98,49,12,0,194,1,155,0,287,0,3,34\n",
      "500,50-50,4,30,0.8987566607460036,0.4771470622123765,7377,50,131,54,109,45,5,0,222,1,127,0,283,0,0,41\n",
      "600,32-16-8,5,20,0.890704558910598,0.47680598009074165,7284,34,191,103,119,32,8,0,217,1,132,0,250,0,0,74\n",
      "600,50-50,3,30,0.8872705743043221,0.47678502765603753,7201,27,287,97,115,22,22,0,154,1,195,0,245,0,4,75\n",
      "500,50-50,4,50,0.8970988750740083,0.4749495584575193,7364,51,124,73,112,42,5,0,229,1,120,0,273,1,0,50\n",
      "600,50-50,4,80,0.897335701598579,0.4736487287026371,7362,38,141,71,113,37,9,0,220,0,130,0,275,0,0,49\n",
      "200,50-50,4,55,0.8902309058614565,0.47308150248708636,7279,53,216,64,111,40,8,0,197,4,149,0,271,0,3,50\n",
      "400,50-50,5,30,0.8978093546477205,0.47186423253376497,7345,34,174,59,121,30,8,0,186,1,163,0,279,0,1,44\n",
      "600,50-25-10,4,80,0.888809946714032,0.4703341165208701,7262,37,249,64,104,36,19,0,190,0,160,0,274,0,2,48\n",
      "400,50-25-10,4,20,0.8904677323860273,0.47029704687367435,7284,32,205,91,122,30,7,0,206,1,142,1,260,0,0,64\n",
      "200,50-50,4,65,0.8954410894020131,0.4690944402196073,7340,76,158,38,104,53,2,0,203,5,142,0,294,3,0,27\n",
      "200,50-50,3,105,0.8988750740082889,0.46687446734869403,7385,49,140,38,114,44,1,0,215,3,132,0,294,0,0,30\n",
      "600,50-50,4,25,0.8889283599763174,0.4665584439611178,7254,32,252,74,110,29,20,0,178,0,172,0,270,2,0,52\n",
      "500,50-50,5,40,0.8978093546477205,0.46648837445614755,7356,31,161,64,121,28,9,1,198,0,152,0,278,0,0,46\n",
      "500,50-25-10,3,75,0.8883362936648904,0.466314907717997,7239,26,270,77,118,24,17,0,169,0,180,1,263,0,2,59\n",
      "400,50-40-30-20-10,5,105,0.900177619893428,0.46449658480921685,7391,30,140,51,130,29,0,0,210,0,140,0,282,0,0,42\n",
      "200,50-25-10,4,60,0.8982830076968621,0.4618600672677168,7376,45,153,38,112,41,6,0,202,2,143,3,295,0,3,26\n",
      "100,50-50,6,20,0.8961515689757252,0.46169637717326945,7348,47,157,60,119,32,8,0,202,2,146,0,279,3,0,42\n",
      "200,50-50,4,35,0.8969804618117229,0.4613258231266657,7361,70,158,23,103,53,3,0,202,4,144,0,303,1,3,17\n",
      "400,50-50,3,30,0.8917702782711664,0.4589148706743214,7307,24,194,87,125,23,11,0,217,0,133,0,256,0,0,68\n",
      "300,50-50,4,40,0.8921255180580225,0.4587637330433221,7319,50,197,46,113,38,8,0,210,2,138,0,284,1,0,39\n",
      "500,50-25-10,4,35,0.8924807578448787,0.4585906189265415,7324,46,207,35,107,42,10,0,209,0,141,0,293,0,1,30\n",
      "300,50-50,4,60,0.8975725281231498,0.45841527675493243,7393,45,104,70,120,36,2,1,252,2,96,0,268,1,0,55\n",
      "\n",
      "Best result Accuracy:0.897335701598579\n",
      "Best result F1:0.4990003631011103\n",
      "Best confusion matrix\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#sorting and list manipulation functions\n",
    "import operator\n",
    "#Operating system functions for interacting with folders and files\n",
    "import codecs\n",
    "import errno\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "#Mathematical functions to handle vectors\n",
    "import numpy as np\n",
    "#Plotting functions for visualizing results\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "#Check if the notebook path is already loaded\n",
    "try:  \n",
    "    noteBookPath\n",
    "except NameError:\n",
    "    print (\"NotebookPath variable dont exist, applying default one\")\n",
    "    noteBookPath= os.getcwd()\n",
    "    pass\n",
    "\n",
    "#Check if dataset path exist\n",
    "datasetFolder=\"dataset\"\n",
    "try:\n",
    "    datasetsPath=os.path.join(noteBookPath,datasetFolder) \n",
    "    if not os.path.isdir(datasetsPath): raise Exception(\"'Dataset' folder dont exist\")\n",
    "    else: \n",
    "        print (\"Directory: '\"+ datasetFolder+\"' exists\")\n",
    "        pass\n",
    "except SystemExit as e:\n",
    "    print(e)\n",
    "\n",
    "#Create experiments folder if not exists\n",
    "dirName=\"experiments\"\n",
    "try:\n",
    "    os.mkdir(os.path.join(noteBookPath,dirName) )\n",
    "    print (\"Directory: '\"+dirName+\"' Created\") \n",
    "except OSError as e:\n",
    "    if e.errno == errno.EEXIST:\n",
    "        print (\"Directory: '\"+ dirName+\"' exists\")\n",
    "        pass\n",
    "    else:\n",
    "        raise \n",
    "except:\n",
    "    print (\"Unexpected error\")\n",
    "    pass  \n",
    "\n",
    "#Check if the experiments folder path exist\n",
    "try:  \n",
    "    experimentsPath\n",
    "except NameError:\n",
    "    print (\"datasetPath variable dont exist, applying default one\")\n",
    "    experimentsPath=os.path.join(noteBookPath,dirName)\n",
    "    pass\n",
    "\n",
    "#Python Script\n",
    "try:\n",
    "    \n",
    "    #Storage all results together in order to get the best ones\n",
    "    totalResults=[]\n",
    "    #Character ngram size\n",
    "    characterNgram=3\n",
    "    #Character embedding size vectors\n",
    "    featureNumbers=[100, 200,300]\n",
    "    #Best results to display\n",
    "    numResults=30\n",
    "    #Labels used to predict the structure of emails\n",
    "    structuralLabels=[\"Body\", \"Body/Intro\", \"Body/Outro\", \"Body/Signature\"]\n",
    "    \n",
    "    #Iterate over each previously created result file\n",
    "    for number in featureNumbers:\n",
    "        \n",
    "        document = \"ResultsNgram-\"+str(characterNgram)+\"-EmbeddingSize-\"+str(number)+\".txt\"  \n",
    "        filePath = os.path.join(experimentsPath,document)\n",
    "        \n",
    "        if (os.path.isfile(filePath)):\n",
    "            \n",
    "            print(\"Get results of document: \"+ document)\n",
    "            \n",
    "            with codecs.open(filePath,\"r\", \"ISO-8859-1\") as file:\n",
    "                #Append each results (and its parameters) \n",
    "                documentResults=[((line.replace('\\n','')).split(\",\")) for line in file]\n",
    "            totalResults.extend(documentResults)\n",
    "    \n",
    "        else: print (\"The following document dont exist: \"+document)\n",
    "            \n",
    "    print(\"\\nTotal number of experiments: \"+str(len(totalResults))) \n",
    "    \n",
    "    #Order the experimental results by Accuracy, TP and TN\n",
    "    bestResults=sorted(totalResults, key=operator.itemgetter(5),reverse=True)\n",
    "    \n",
    "    print(\"\\nCharacter embedding best experiments according to F1 values\")\n",
    "    #Print ordered results\n",
    "    for values in bestResults[:numResults]:\n",
    "        print(\",\".join(values))      \n",
    "    \n",
    "    bestResult=bestResults[0]  \n",
    "    \n",
    "    print(\"\\nBest result Accuracy:\"+str(bestResult[4]))\n",
    "    print(\"Best result F1:\"+str(bestResult[5]))\n",
    "    print(\"Best confusion matrix\")\n",
    "    cm= (np.array([int(x) for x in bestResult[6:]])).reshape(4, 4)\n",
    "    #Plot the best confusion matrix\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Character embedding best experiment')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(structuralLabels))\n",
    "    plt.xticks(tick_marks, structuralLabels, rotation=45)\n",
    "    plt.yticks(tick_marks, structuralLabels)\n",
    "    fmt = '.2f' if False else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]),   range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "except IOError as e:\n",
    "    print (\"Could not read file\")\n",
    "    print(traceback.format_exc())\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print (\"Unexpected error\")\n",
    "    print(traceback.format_exc())\n",
    "    print(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future/Alternative work\n",
    "\n",
    "Future research avenues includes some of the following:\n",
    "\n",
    "\n",
    "1\\. Analyze other dataset options for improving the model creation of the friend/foe classifier.\n",
    "\n",
    "2\\. Use other test data samples for checking the overall performance of the classifier over distinct types of emails.\n",
    "\n",
    "3\\. Use different supervised learning algorithms to improve classification (see: https://scikit-learn.org/stable/supervised_learning.html).\n",
    "\n",
    "4\\. Use other Neural Network architectures and methods to improve results.\n",
    "\n",
    "5\\. Improve error handling in the implemented Python code.\n",
    "\n",
    "6\\. Considering that are millions of emails/texts in the project (very likely) (https://ased.io/) it is necessary to use parallel techniques to distribute the classification work (see: https://spark.apache.org/ or http://docs.dask.org/en/latest/why.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
